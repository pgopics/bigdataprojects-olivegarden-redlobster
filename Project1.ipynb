{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8152fc12-35d6-4d82-b188-afe0e126405f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data_2018 = pd.read_csv(\"weekly_patterns_2018_sample.csv\")\n",
    "#data_2018\n",
    "data_2019 = pd.read_csv(\"weekly_patterns_2019_sample.csv\")\n",
    "#data_2019\n",
    "data_2020 = pd.read_csv(\"weekly_patterns_2020_sample.csv\")\n",
    "#data_2020\n",
    "data_2021 = pd.read_csv(\"weekly_patterns_2021_sample.csv\")\n",
    "#data_2021\n",
    "data_2022 = pd.read_csv(\"weekly_patterns_2022_sample.csv\")\n",
    "#data_2022\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9991a0dd-37be-4107-9bbb-43d1c524a33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5165737\n"
     ]
    }
   ],
   "source": [
    "#1a) how many observations do you have in total in all datasets? #5165737\n",
    "#What is the unit of observation in the data?\n",
    "obs_2018 = len(data_2018)\n",
    "obs_2019 = len(data_2019)\n",
    "obs_2020 = len(data_2020)\n",
    "obs_2021  = len(data_2021)\n",
    "obs_2022 = len(data_2022)\n",
    "total_obs = obs_2018 = obs_2019 + obs_2020 + obs_2021 + obs_2022\n",
    "print(total_obs)\n",
    "#unit of observation -> placekey\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a50b19d6-00ba-43bf-aeeb-0d36b569246b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50578\n"
     ]
    }
   ],
   "source": [
    "#1b first condition on your resturant change for each of the datasets, \n",
    "#and then concatenate them together. how many observations do you have now? \n",
    "\n",
    "df_2018 = data_2018[data_2018['brands'] == 'Olive Garden']\n",
    "df_2019 = data_2019[data_2019['brands'] == 'Olive Garden']\n",
    "df_2020 = data_2020[data_2020['brands'] == 'Olive Garden']\n",
    "df_2021 = data_2021[data_2021['brands'] == 'Olive Garden']\n",
    "df_2022 = data_2022[data_2022['brands'] == 'Olive Garden']\n",
    "wide_sample = pd.concat([df_2018, df_2019, df_2020, df_2021,df_2022])\n",
    "print(len(wide_sample))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07f9abc9-b0da-4d65-ac94-93ffb899d624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "placekey               object\n",
      "city                   object\n",
      "region                 object\n",
      "date_range_start       object\n",
      "date_range_end         object\n",
      "raw_visit_counts        int64\n",
      "visits_by_day          object\n",
      "safegraph_brand_ids    object\n",
      "naics_code              int64\n",
      "postal_code             int64\n",
      "brands                 object\n",
      "dtype: object\n",
      "201\n"
     ]
    }
   ],
   "source": [
    "# 1c from now on concatenated data of resturant chain is wide sample. what\n",
    "# is the data type of each variable in your wide sample? How many unique places\n",
    "#do you observe for your chain in the US? is it consistent with the \n",
    "#number of units your chain should have if you search for this number online?\n",
    "#comment on your findings. Try to come up with at least one explanation if \n",
    "# the two numbers are different. \n",
    "\n",
    "print(wide_sample.dtypes)\n",
    "number_of_unique_places = wide_sample['placekey'].nunique()\n",
    "print(number_of_unique_places) #201\n",
    "\n",
    "#As of the end of 2022 there are 884 Olive Garden resturants via online.\n",
    "#My findings say 201, A reason the two numbers may be different could\n",
    "#be because of the changes that could have happened due to the pandemic, \n",
    "#causing a decline in the resturant industry, which resulted \n",
    "#in a lot of resturants #shutting down, the online value could have \n",
    "#failed to properly account for this and as a result, has a greater value \n",
    "#than my findings for the number of unique Olive Garden's open. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4017216-23d4-4ab8-9993-572e74c5b794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Date: 2018-01-01 00:00:00+00:00\n",
      "Ending Date: 2023-01-02 00:00:00+00:00\n"
     ]
    }
   ],
   "source": [
    "#now save your wide data to a .csv file\n",
    "wide_sample.to_csv('wide_sample.csv',index = False)\n",
    "\n",
    "#part 2) clean your wide_sample and reshape it into \"long sample\" \n",
    "\n",
    "#Question 2a (1 point): In the raw data, the date variables are string variables. \n",
    "#Convert them into python’s datetime format, in unit \"day\". \n",
    "#What are the beginning date and ending date in your “wide sample”?\n",
    "\n",
    "wide_sample['date_range_start'] = pd.to_datetime(wide_sample['date_range_start'],utc = True).dt.normalize()\n",
    "wide_sample['date_range_end']=  pd.to_datetime(wide_sample['date_range_end'], utc = True).dt.normalize()\n",
    "\n",
    "# Find the beginning and ending dates\n",
    "beginning_date = wide_sample['date_range_start'].min()\n",
    "ending_date = wide_sample['date_range_end'].max()\n",
    "\n",
    "print(\"Beginning Date:\", beginning_date)\n",
    "print(\"Ending Date:\", ending_date)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e813168b-23ed-4b87-b75a-1248d3f42b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>placekey</th>\n",
       "      <th>city</th>\n",
       "      <th>region</th>\n",
       "      <th>date_range_start</th>\n",
       "      <th>date_range_end</th>\n",
       "      <th>raw_visit_counts</th>\n",
       "      <th>visits_by_day</th>\n",
       "      <th>safegraph_brand_ids</th>\n",
       "      <th>naics_code</th>\n",
       "      <th>postal_code</th>\n",
       "      <th>brands</th>\n",
       "      <th>daily_visits1</th>\n",
       "      <th>daily_visits2</th>\n",
       "      <th>daily_visits3</th>\n",
       "      <th>daily_visits4</th>\n",
       "      <th>daily_visits5</th>\n",
       "      <th>daily_visits6</th>\n",
       "      <th>daily_visits7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>82311</th>\n",
       "      <td>222-222@8dy-ssr-z2k</td>\n",
       "      <td>Bayou Cane</td>\n",
       "      <td>LA</td>\n",
       "      <td>2018-01-01 00:00:00+00:00</td>\n",
       "      <td>2018-01-08 00:00:00+00:00</td>\n",
       "      <td>170</td>\n",
       "      <td>[25,12,12,23,41,38,19]</td>\n",
       "      <td>SG_BRAND_29e52fe03f73e6ce21a527123c4d97b0</td>\n",
       "      <td>722511</td>\n",
       "      <td>70360</td>\n",
       "      <td>Olive Garden</td>\n",
       "      <td>25</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>23</td>\n",
       "      <td>41</td>\n",
       "      <td>38</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82312</th>\n",
       "      <td>222-222@5s6-9w8-skf</td>\n",
       "      <td>Mishawaka</td>\n",
       "      <td>IN</td>\n",
       "      <td>2018-11-19 00:00:00+00:00</td>\n",
       "      <td>2018-11-26 00:00:00+00:00</td>\n",
       "      <td>167</td>\n",
       "      <td>[19,21,25,1,52,26,23]</td>\n",
       "      <td>SG_BRAND_29e52fe03f73e6ce21a527123c4d97b0</td>\n",
       "      <td>722511</td>\n",
       "      <td>46545</td>\n",
       "      <td>Olive Garden</td>\n",
       "      <td>19</td>\n",
       "      <td>21</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>52</td>\n",
       "      <td>26</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82313</th>\n",
       "      <td>222-222@5qs-zmp-gc5</td>\n",
       "      <td>Oklahoma City</td>\n",
       "      <td>OK</td>\n",
       "      <td>2018-03-26 00:00:00+00:00</td>\n",
       "      <td>2018-04-02 00:00:00+00:00</td>\n",
       "      <td>223</td>\n",
       "      <td>[31,30,24,25,35,41,37]</td>\n",
       "      <td>SG_BRAND_29e52fe03f73e6ce21a527123c4d97b0</td>\n",
       "      <td>722511</td>\n",
       "      <td>73118</td>\n",
       "      <td>Olive Garden</td>\n",
       "      <td>31</td>\n",
       "      <td>30</td>\n",
       "      <td>24</td>\n",
       "      <td>25</td>\n",
       "      <td>35</td>\n",
       "      <td>41</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82314</th>\n",
       "      <td>222-222@5xq-dzd-b8v</td>\n",
       "      <td>Santa Maria</td>\n",
       "      <td>CA</td>\n",
       "      <td>2018-01-15 00:00:00+00:00</td>\n",
       "      <td>2018-01-22 00:00:00+00:00</td>\n",
       "      <td>121</td>\n",
       "      <td>[22,17,13,10,20,18,21]</td>\n",
       "      <td>SG_BRAND_29e52fe03f73e6ce21a527123c4d97b0</td>\n",
       "      <td>722511</td>\n",
       "      <td>93454</td>\n",
       "      <td>Olive Garden</td>\n",
       "      <td>22</td>\n",
       "      <td>17</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>18</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82315</th>\n",
       "      <td>222-222@62k-p8d-jqf</td>\n",
       "      <td>Stoughton</td>\n",
       "      <td>MA</td>\n",
       "      <td>2018-05-14 00:00:00+00:00</td>\n",
       "      <td>2018-05-21 00:00:00+00:00</td>\n",
       "      <td>107</td>\n",
       "      <td>[6,4,6,9,24,34,24]</td>\n",
       "      <td>SG_BRAND_29e52fe03f73e6ce21a527123c4d97b0</td>\n",
       "      <td>722511</td>\n",
       "      <td>2072</td>\n",
       "      <td>Olive Garden</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>24</td>\n",
       "      <td>34</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  placekey           city region          date_range_start  \\\n",
       "82311  222-222@8dy-ssr-z2k     Bayou Cane     LA 2018-01-01 00:00:00+00:00   \n",
       "82312  222-222@5s6-9w8-skf      Mishawaka     IN 2018-11-19 00:00:00+00:00   \n",
       "82313  222-222@5qs-zmp-gc5  Oklahoma City     OK 2018-03-26 00:00:00+00:00   \n",
       "82314  222-222@5xq-dzd-b8v    Santa Maria     CA 2018-01-15 00:00:00+00:00   \n",
       "82315  222-222@62k-p8d-jqf      Stoughton     MA 2018-05-14 00:00:00+00:00   \n",
       "\n",
       "                 date_range_end  raw_visit_counts           visits_by_day  \\\n",
       "82311 2018-01-08 00:00:00+00:00               170  [25,12,12,23,41,38,19]   \n",
       "82312 2018-11-26 00:00:00+00:00               167   [19,21,25,1,52,26,23]   \n",
       "82313 2018-04-02 00:00:00+00:00               223  [31,30,24,25,35,41,37]   \n",
       "82314 2018-01-22 00:00:00+00:00               121  [22,17,13,10,20,18,21]   \n",
       "82315 2018-05-21 00:00:00+00:00               107      [6,4,6,9,24,34,24]   \n",
       "\n",
       "                             safegraph_brand_ids  naics_code  postal_code  \\\n",
       "82311  SG_BRAND_29e52fe03f73e6ce21a527123c4d97b0      722511        70360   \n",
       "82312  SG_BRAND_29e52fe03f73e6ce21a527123c4d97b0      722511        46545   \n",
       "82313  SG_BRAND_29e52fe03f73e6ce21a527123c4d97b0      722511        73118   \n",
       "82314  SG_BRAND_29e52fe03f73e6ce21a527123c4d97b0      722511        93454   \n",
       "82315  SG_BRAND_29e52fe03f73e6ce21a527123c4d97b0      722511         2072   \n",
       "\n",
       "             brands  daily_visits1  daily_visits2  daily_visits3  \\\n",
       "82311  Olive Garden             25             12             12   \n",
       "82312  Olive Garden             19             21             25   \n",
       "82313  Olive Garden             31             30             24   \n",
       "82314  Olive Garden             22             17             13   \n",
       "82315  Olive Garden              6              4              6   \n",
       "\n",
       "       daily_visits4  daily_visits5  daily_visits6  daily_visits7  \n",
       "82311             23             41             38             19  \n",
       "82312              1             52             26             23  \n",
       "82313             25             35             41             37  \n",
       "82314             10             20             18             21  \n",
       "82315              9             24             34             24  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Question 2b (1 point): In the raw data, the variable visits_by_day is text only. \n",
    "#It contains a string of list, consists of multiple integers separated by comma \n",
    "#(e.g. “[3,2,4,0,5,2,1]”). Please split the variable into new variables \n",
    "#(dailyvisits1, …, dailyvisits7), each of which is an integer. \n",
    "#How many variables do you have in the “wide sample” now?\n",
    "\n",
    "#Hint: you may need to create 7 new columns to store the split data from visits_by_day.\n",
    "\n",
    "#splitting visit by day array for week to seperate days by replacing [] with ''\n",
    "# and splitting by commas\n",
    "visits_by_day_lists = wide_sample['visits_by_day'].str.replace('[\\[\\]]', '', regex=True).str.split(',', expand=True)\n",
    "visits_by_day_lists\n",
    "\n",
    "for i in range(7):\n",
    "    wide_sample[f'daily_visits{i+1}'] = visits_by_day_lists[i].astype(int)\n",
    "\n",
    "print(len(wide_sample.columns))  #18 including the 7 new samples and the original\n",
    "                                #visits_by_day column 17 excluding original visits_by_day column\n",
    "wide_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2c0b090-fd60-4299-8bd5-f45ab0af4bec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "354046"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Question 2c (2 points): Reshape your “wide sample” so that each \n",
    "#observation is a place-date instead of a place-week. Now all the \n",
    "#7 variables (dailyvisits1, …, dailyvisits7) should consolidate into \n",
    "#one variable (dailyvisits). I will call the reshaped data your “long sample”.\n",
    "#How many observations do you have in the “long sample”? Please comment \n",
    "#on the pros and cons of the “long sample” relative to the “wide sample”.\n",
    "\n",
    "#wide_sample.columns\n",
    "\n",
    "# Define the id_vars (these columns will not be melted)\n",
    "id_vars = ['placekey', 'city', 'region', 'date_range_start', 'date_range_end',\n",
    "           'raw_visit_counts', 'visits_by_day', 'safegraph_brand_ids',\n",
    "           'naics_code', 'postal_code', 'brands']\n",
    "\n",
    "# Define the value_vars (these are the daily visits columns to be melted)\n",
    "value_vars = ['daily_visits1', 'daily_visits2', 'daily_visits3', 'daily_visits4',\n",
    "              'daily_visits5', 'daily_visits6', 'daily_visits7']\n",
    "\n",
    "long_sample = wide_sample.melt(id_vars=id_vars, value_vars=value_vars, var_name='day', value_name='dailyvisits')\n",
    "len(long_sample) #354046 observations in long sample\n",
    "long_sample.head()\n",
    "\n",
    "\n",
    "#The pros of the long sample relative to wide sample includes the fact that a long \n",
    "#sample is better for data that is collected from the same place,\n",
    "#in this case, Olive Garden, over multiple timestamps.\n",
    "#Long data is easier to modify variables.\n",
    "#However, a con of the long sample in relative to\n",
    "#wide sample is that the datesets are a lot larger than the datasets\n",
    "#of wide samples becausethe long sample has repeating values, which means \n",
    "#that it is slower and takes a longer time to run. \n",
    "\n",
    "#Convert the dailyvisits variable into an integer variable directly. \n",
    "#Does it work? If not, explain why and find a way to address the problem.\n",
    "#How many observations do you have now after you resolve the issue? \n",
    "\n",
    "#It works for me because I already took care of the character/bracket [] issue so \n",
    "#dailyvisits was int32 for me by default. I didn't encouter any conversion issue\n",
    "#so the amount of observations remains the same.\n",
    "\n",
    "long_sample.dtypes\n",
    "len(long_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d968347c-8ba6-4432-90cb-ed99faee79de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 32.9217474565452\n",
      "Standard Deviation: 21.625747087523784\n",
      "Minimum: 0.0\n",
      "Maximum: 269.0\n",
      "Based on my definition of an outlier there are 4898 outliers identified in the 'dailyvisits' variable.\n"
     ]
    }
   ],
   "source": [
    "#2d) In your \"long sample\" describe the variable dailyvisits. what are the mean\n",
    "#standard deviation, minimum, and maximum of this variable? comment on the\n",
    "#presence and the extent of outliers. you can use whatever definition of \"outlier\", \n",
    "#as long as you state the definition clearly\n",
    "\n",
    "summary_stats = long_sample['dailyvisits'].describe()\n",
    "summary_stats\n",
    "\n",
    "mean_daily_visits = summary_stats['mean']\n",
    "std_daily_visits = summary_stats['std']\n",
    "min_daily_visits = summary_stats['min']\n",
    "max_daily_visits = summary_stats['max']\n",
    "\n",
    "print(\"Mean:\", mean_daily_visits)\n",
    "print(\"Standard Deviation:\", std_daily_visits)\n",
    "print(\"Minimum:\", min_daily_visits)\n",
    "print(\"Maximum:\", max_daily_visits)\n",
    "\n",
    "\n",
    "#my definition of outlier: values that are more than 3 standard deviations\n",
    "#away from the mean.\n",
    "threshold_upper = mean_daily_visits + 3 * std_daily_visits\n",
    "threshold_lower = mean_daily_visits - 3 * std_daily_visits\n",
    "\n",
    "outliers_upper = long_sample[long_sample['dailyvisits'] > threshold_upper]\n",
    "outliers_lower = long_sample[long_sample['dailyvisits'] < threshold_lower]\n",
    "\n",
    "outliers = pd.concat([outliers_upper, outliers_lower])\n",
    "\n",
    "print(\"Based on my definition of an outlier there are\", len(outliers), \n",
    "      \"outliers identified in the 'dailyvisits' variable.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0bc1d877-a484-46e6-9b9b-82557a9a9bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "#2d continued:  also comment on missing values --- to what extent do you observe\n",
    "#missing values in dailyvisits? what could be the economic reasons for these \n",
    "#missing values? how can you use your data to tell whether these reasons are \n",
    "#relevant for your data? \n",
    "\n",
    "missing_values_count = long_sample.loc[long_sample['dailyvisits'].isna()].shape[0]\n",
    "print(missing_values_count)\n",
    "\n",
    "#do not have any missing values in daily visits. potential economic reasons for\n",
    "#missing values could include the pandemic time 2020-2022 as less people went \n",
    "#out to eat, economic conditions such as recessions also affected afforability \n",
    "#and priorities for different consumers which could have \n",
    "#influenced the missing value count as well\n",
    "\n",
    "#how would outliers and/or missing values affect the quality of the dataset when\n",
    "#you, as a businessmanager of the chain, attempt to make business decisions\n",
    "#based on this dataset? name at least onedecisions that might be immune\n",
    "#to these data issues and another one that might be very senstive to these\n",
    "#data issues.\n",
    "\n",
    "#outliers/missing values can greatly affect the quality of the dataset as \n",
    "#they can overestimate/ critical measurements: mean, standard deviation and\n",
    "#other statistical indicators. This can lead to flawed business decisions \n",
    "#from business managers esp. when trying to forecast short\n",
    "#term revenue/sales trends for a resturant chain's like Olive Garden. \n",
    "#For example, a decision that could be highly sensitive to outliers and \n",
    "# missing values is staffing allocation. this is because \n",
    "#outliers/missing values in how many customers visit Olive Garden for \n",
    "#certain days could lead to  underestimating/overestimating the amount\n",
    "#of customers that will visit Olive Garden on other days. \n",
    "#this in turn can lead to poor customer service, which can negatively \n",
    "#impact sales and revenue. However, these data issues are more immune to \n",
    "#long term trends - using past data from several years \n",
    "#ago can still provide notable insights about current/future business\n",
    "#decisions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "790e40b3-4dbc-4d8d-bb83-270a6bbb728d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>start_date</th>\n",
       "      <th>end_date</th>\n",
       "      <th>observations</th>\n",
       "      <th>mean_daily_visits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018</td>\n",
       "      <td>2018-01-01 00:00:00+00:00</td>\n",
       "      <td>2019-01-07 00:00:00+00:00</td>\n",
       "      <td>72135</td>\n",
       "      <td>28.980218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019</td>\n",
       "      <td>2019-01-07 00:00:00+00:00</td>\n",
       "      <td>2020-01-06 00:00:00+00:00</td>\n",
       "      <td>70882</td>\n",
       "      <td>38.230707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020</td>\n",
       "      <td>2020-01-06 00:00:00+00:00</td>\n",
       "      <td>2021-01-04 00:00:00+00:00</td>\n",
       "      <td>70455</td>\n",
       "      <td>27.061529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021</td>\n",
       "      <td>2021-01-04 00:00:00+00:00</td>\n",
       "      <td>2022-01-03 00:00:00+00:00</td>\n",
       "      <td>70497</td>\n",
       "      <td>35.291473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022</td>\n",
       "      <td>2022-01-03 00:00:00+00:00</td>\n",
       "      <td>2023-01-02 00:00:00+00:00</td>\n",
       "      <td>70077</td>\n",
       "      <td>35.116986</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year                start_date                  end_date  observations  \\\n",
       "0  2018 2018-01-01 00:00:00+00:00 2019-01-07 00:00:00+00:00         72135   \n",
       "1  2019 2019-01-07 00:00:00+00:00 2020-01-06 00:00:00+00:00         70882   \n",
       "2  2020 2020-01-06 00:00:00+00:00 2021-01-04 00:00:00+00:00         70455   \n",
       "3  2021 2021-01-04 00:00:00+00:00 2022-01-03 00:00:00+00:00         70497   \n",
       "4  2022 2022-01-03 00:00:00+00:00 2023-01-02 00:00:00+00:00         70077   \n",
       "\n",
       "   mean_daily_visits  \n",
       "0          28.980218  \n",
       "1          38.230707  \n",
       "2          27.061529  \n",
       "3          35.291473  \n",
       "4          35.116986  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#part 3 \n",
    "#now you summarize the \"long sample\" \n",
    "\n",
    "#3a) aggregate your long sample to generate a table that summarizes\n",
    "#the starting and ending dates of each year, # of observations in each \n",
    "#year, and the mean of dailyvisits of each year\n",
    "\n",
    "long_sample['year'] = long_sample['date_range_start'].dt.year\n",
    "\n",
    "# Group by the extracted year and aggregate\n",
    "summary = long_sample.groupby('year').agg(\n",
    "    start_date=('date_range_start', 'min'),\n",
    "    end_date=('date_range_end', 'max'),\n",
    "    observations=('dailyvisits', 'size'),\n",
    "    mean_daily_visits=('dailyvisits', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6ee760ca-2481-412a-acfd-515008eae758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   dayofweek  unique_days  observations  mean_daily_visits\n",
      "0     Friday            1         50578          40.112934\n",
      "1     Monday            1         50578          25.956859\n",
      "2   Saturday            1         50578          45.996718\n",
      "3     Sunday            1         50578          38.964431\n",
      "4   Thursday            1         50578          27.943473\n",
      "5    Tuesday            1         50578          24.901321\n",
      "6  Wednesday            1         50578          26.576496\n",
      " day with the highest: Saturday\n"
     ]
    }
   ],
   "source": [
    "#Question 3b (1 point):  Define a new variable “dayofweek”. \n",
    "#You can define the variable as numeric, with the value 1 for \n",
    "#Monday, and …7 for Sunday; or as text with the values being \n",
    "#“Monday”,..,”Sunday”. (Hint: you need to create a date variable \n",
    "#before this step. The date variable should give you the\n",
    "#exact date of an observation. It can be created using the \n",
    "#date_range_start variable and the  variable you created when\n",
    "#you reshape your data (the 'j' variable).\n",
    "\n",
    "#Aggregate dailyvisits by dayofweek in your long sample – that is, \n",
    "#generate a table that tells you of unique values in dayofweek, #\n",
    "#of observations per value, and the mean of dailyvisits per value \n",
    "#of dayofweek. On average, which day of week has the highest\n",
    "#dailyvisits in your long sample?\n",
    "\n",
    "long_sample['dayofweek'] = long_sample['day'].str[-1].astype(int)\n",
    "\n",
    "days_of_week = {1: 'Monday', 2: 'Tuesday',\n",
    "                3: 'Wednesday', 4: 'Thursday',\n",
    "                5: 'Friday', 6: 'Saturday', 7: 'Sunday'}\n",
    "long_sample['dayofweek'] = long_sample['dayofweek'].map(days_of_week)\n",
    "\n",
    "summary_by_dayofweek = long_sample.groupby('dayofweek').agg(\n",
    "    unique_days=('dayofweek', 'nunique'),\n",
    "    observations=('dailyvisits', 'size'),\n",
    "    mean_daily_visits=('dailyvisits', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "highest_visits_day = summary_by_dayofweek.loc[summary_by_dayofweek['mean_daily_visits'].idxmax()]\n",
    "\n",
    "print(summary_by_dayofweek)\n",
    "print(f\" day with the highest: {highest_visits_day['dayofweek']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6de96d93-5dee-4caf-9bca-310cebccec13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     postal_code  sum  count      mean  percentage_manyvisits\n",
      "0           1119   51    826  0.061743               0.061743\n",
      "1           1453  109   1827  0.059661               0.059661\n",
      "2           1607  140   1827  0.076628               0.076628\n",
      "3           1701  112   1827  0.061303               0.061303\n",
      "4           2072  101   1827  0.055282               0.055282\n",
      "..           ...  ...    ...       ...                    ...\n",
      "195        98373  878   1827  0.480569               0.480569\n",
      "196        98408  591   1827  0.323481               0.323481\n",
      "197        98662  522   1827  0.285714               0.285714\n",
      "198        98802  758   1827  0.414888               0.414888\n",
      "199        99218  426   1827  0.233169               0.233169\n",
      "\n",
      "[200 rows x 5 columns]\n",
      "Postal code with the highest number: 30144\n",
      "Postal code with the highest percentage: 30144\n"
     ]
    }
   ],
   "source": [
    "#3c:define a new variable 'manyvisits' equal to 1 if dailyvisits \n",
    "#is above a threshold and 0 otherwise.\n",
    "#You can choose whatever threshold you like, it could be a constant \n",
    "#or a function that depends on \n",
    "#other variables in your data (e.g. the threshold may vary by season, \n",
    "#weekend, or geography). Explain\n",
    "#why you choose this threshold.\n",
    "\n",
    "# I am going to make the threshold the mean of dailyvisits. \n",
    "#I chose this threshold because personally\n",
    "# speaking I would define many as something that is bigger than the average.\n",
    "\n",
    "threshold = long_sample['dailyvisits'].mean()\n",
    "\n",
    "long_sample['manyvisits'] = (long_sample['dailyvisits'] > threshold).astype(int)\n",
    "\n",
    "\n",
    "#Tabulate the variable manyvisits and one geographic variable of your choice \n",
    "#(e.g. the geographic variable can be state, zipcode, or a new variable of your own \n",
    "#definition). By tabulation, I mean generating a table that tells you # of \n",
    "#observations in each possible combination of your geographic variable\n",
    "#and manyvisits. Which geographic unit has the highest # of observations \n",
    "#with manyvisits=1? Which geographic unit has the highest percentage \n",
    "#of observations with manyvisits=1?\n",
    "\n",
    "#geographic variable of my choice is postal_code: postal_code\n",
    "tabulation = long_sample.groupby('postal_code')['manyvisits'].agg(['sum', 'count', 'mean']).reset_index()\n",
    "#percentage\n",
    "tabulation['percentage_manyvisits'] = tabulation['sum'] / tabulation['count']\n",
    "\n",
    "#which postal_code has the highest number of observations with manyvisits = 1\n",
    "max_manyvisits = tabulation.loc[tabulation['sum'].idxmax()]\n",
    "\n",
    "#which post_code has the highest percentage of observations with manyvisits = 1\n",
    "highest_percentage = tabulation.loc[tabulation['percentage_manyvisits'].idxmax()]\n",
    "\n",
    "print(tabulation)\n",
    "print(f\"Postal code with the highest number: {max_manyvisits['postal_code'].astype(int)}\")\n",
    "print(f\"Postal code with the highest percentage: {highest_percentage['postal_code'].astype(int)}\")\n",
    "\n",
    "\n",
    "#Do these findings make sense to you given the public information \n",
    "#you can find online about your\n",
    "#restaurant chain? \n",
    "#(e.g. if you find that California has the highest # of observations\n",
    "#with manyvisits=1 but the public information indicates that your chain\n",
    "#never operates in California, then something must be wrong. \n",
    "#Figureout why.)\n",
    "\n",
    "#Yes, these findings make sense. there is a Olive Garden in zipcode 30144\n",
    "#located in Kennesaw, GA opposite to the Town Center Mall.\n",
    "#A reason the Olive Garden in Kennesaw, GA could have the highest\n",
    "#number and percentage of manyvists = 1 could be because of its location,\n",
    "#many consumers will be hungry after spending long hours at the Town Center Mall,\n",
    "#and will naturally go to a resturant that is close in proxmity to eat. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bcd78536-53a8-4ce2-a67c-0b8e748e6639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   core_biz_area  observations  mean_daily_visits\n",
      "0              0        101073          28.729314\n",
      "1              1        252973          34.596795\n"
     ]
    }
   ],
   "source": [
    "#3d) Question 3d (2 point): Define a new variable “core_biz_area” equal to 1 if the\n",
    "#core business areas of your chain, and 0 otherwise. You can define core business \n",
    "#areas whatever way you like, but please explain why you define it this way.\n",
    "#(For example, you can define all states that have at least X units\n",
    "#in your chain as the core business area and explain why you choose a specific\n",
    "#number for X). \n",
    "\n",
    "region_frequency = long_sample['region'].value_counts()\n",
    "region_frequency\n",
    "\n",
    "threshold = region_frequency.mean()\n",
    "\n",
    "# Define the core business areas based on the threshold\n",
    "# the regions above the mean is 1 and the regions under the mean is 0\n",
    "long_sample['core_biz_area'] = long_sample['region'].apply(lambda x: 1 if region_frequency[x] >= threshold else 0)\n",
    "\n",
    "# I chose the mean of the region frequency as my main threshold factor that \n",
    "#determines the core business areas as I thought that the higher the \n",
    "#frequency the more value the business has in said area. So\n",
    "# since core to me is synonomous to \"prominent\" I decided to draw \n",
    "#the mean region region frequency as the threshold factor that determines \n",
    "# core business areas. Regions with higher region frequency than \n",
    "#the mean are more promiment and as a result are more \"core\"\n",
    "\n",
    "\n",
    "#Aggregate the data by core_biz_area to show the # of observations and \n",
    "#mean dailyvisits in core areas vs non-core areas. On average, do you\n",
    "# observe more or fewer daily visits in the core areas?\n",
    "#Could you name at least two reasons that may explain your finding?\n",
    "\n",
    "# Aggregate the data by core_biz_area\n",
    "aggregate_by_core = long_sample.groupby('core_biz_area').agg(\n",
    "    observations=('dailyvisits', 'size'),\n",
    "    mean_daily_visits=('dailyvisits', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "print(aggregate_by_core)\n",
    "\n",
    "#there are more observations and a higher average daily visit ratio in \n",
    "#core areas relative to non-core areas.Two reasons that can explain\n",
    "#this finding could be higher population density and repeated\n",
    "#customers in core areas as well as location. Core areas could \n",
    "#also have better customer service, enhanced menu options, etc. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
