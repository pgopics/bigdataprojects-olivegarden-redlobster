{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "223b19f8-ce84-4287-afd0-a01fe8abd453",
   "metadata": {},
   "source": [
    "Question 1a (1 point): Use MergedSummary you have created at the end of Project 3. Display a table showing the number of observations for each state in this data for your own chain and your benchmark chain respectively. It is ok to use multiple tables if that is more convenient for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "32a06edc-9313-4f0f-b918-fa16163895ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   placekey            city region           date_range_start  \\\n",
      "0       222-222@8dy-ssr-z2k      Bayou Cane     LA  2018-01-01 00:00:00+00:00   \n",
      "1       222-222@5s6-9w8-skf       Mishawaka     IN  2018-11-19 00:00:00+00:00   \n",
      "2       222-222@5qs-zmp-gc5   Oklahoma City     OK  2018-03-26 00:00:00+00:00   \n",
      "3       222-222@5xq-dzd-b8v     Santa Maria     CA  2018-01-15 00:00:00+00:00   \n",
      "4       222-222@62k-p8d-jqf       Stoughton     MA  2018-05-14 00:00:00+00:00   \n",
      "...                     ...             ...    ...                        ...   \n",
      "354041  22h-222@629-38p-4jv   West Hartford     CT  2022-12-05 00:00:00+00:00   \n",
      "354042  222-222@63k-b4h-cqz      Fort Wayne     IN  2022-02-21 00:00:00+00:00   \n",
      "354043  22b-222@5qv-tcj-gp9         Prosper     TX  2022-09-12 00:00:00+00:00   \n",
      "354044  222-223@5z4-ktq-7dv  San Bernardino     CA  2022-10-24 00:00:00+00:00   \n",
      "354045  224-222@5qw-stn-s3q        Mesquite     TX  2022-03-14 00:00:00+00:00   \n",
      "\n",
      "                   date_range_end  raw_visit_counts            visits_by_day  \\\n",
      "0       2018-01-08 00:00:00+00:00               170   [25,12,12,23,41,38,19]   \n",
      "1       2018-11-26 00:00:00+00:00               167    [19,21,25,1,52,26,23]   \n",
      "2       2018-04-02 00:00:00+00:00               223   [31,30,24,25,35,41,37]   \n",
      "3       2018-01-22 00:00:00+00:00               121   [22,17,13,10,20,18,21]   \n",
      "4       2018-05-21 00:00:00+00:00               107       [6,4,6,9,24,34,24]   \n",
      "...                           ...               ...                      ...   \n",
      "354041  2022-12-12 00:00:00+00:00               211   [15,15,20,19,39,71,32]   \n",
      "354042  2022-02-28 00:00:00+00:00               456  [42,38,56,35,88,120,77]   \n",
      "354043  2022-09-19 00:00:00+00:00               303   [28,24,34,46,57,68,46]   \n",
      "354044  2022-10-31 00:00:00+00:00               244   [40,26,34,24,44,31,45]   \n",
      "354045  2022-03-21 00:00:00+00:00               544   [62,77,66,60,97,92,90]   \n",
      "\n",
      "                              safegraph_brand_ids  naics_code  postal_code  \\\n",
      "0       SG_BRAND_29e52fe03f73e6ce21a527123c4d97b0      722511        70360   \n",
      "1       SG_BRAND_29e52fe03f73e6ce21a527123c4d97b0      722511        46545   \n",
      "2       SG_BRAND_29e52fe03f73e6ce21a527123c4d97b0      722511        73118   \n",
      "3       SG_BRAND_29e52fe03f73e6ce21a527123c4d97b0      722511        93454   \n",
      "4       SG_BRAND_29e52fe03f73e6ce21a527123c4d97b0      722511         2072   \n",
      "...                                           ...         ...          ...   \n",
      "354041  SG_BRAND_29e52fe03f73e6ce21a527123c4d97b0      722511         6110   \n",
      "354042  SG_BRAND_29e52fe03f73e6ce21a527123c4d97b0      722511        46805   \n",
      "354043  SG_BRAND_29e52fe03f73e6ce21a527123c4d97b0      722511        75078   \n",
      "354044  SG_BRAND_29e52fe03f73e6ce21a527123c4d97b0      722511        92408   \n",
      "354045  SG_BRAND_29e52fe03f73e6ce21a527123c4d97b0      722511        75150   \n",
      "\n",
      "              brands            day  dailyvisits dayofweek  manyvisits  \\\n",
      "0       Olive Garden  daily_visits1           25    Monday           0   \n",
      "1       Olive Garden  daily_visits1           19    Monday           0   \n",
      "2       Olive Garden  daily_visits1           31    Monday           0   \n",
      "3       Olive Garden  daily_visits1           22    Monday           0   \n",
      "4       Olive Garden  daily_visits1            6    Monday           0   \n",
      "...              ...            ...          ...       ...         ...   \n",
      "354041  Olive Garden  daily_visits7           32    Sunday           0   \n",
      "354042  Olive Garden  daily_visits7           77    Sunday           1   \n",
      "354043  Olive Garden  daily_visits7           46    Sunday           1   \n",
      "354044  Olive Garden  daily_visits7           45    Sunday           1   \n",
      "354045  Olive Garden  daily_visits7           90    Sunday           1   \n",
      "\n",
      "        core_biz_area  weekend  \n",
      "0                   1        0  \n",
      "1                   1        0  \n",
      "2                   1        0  \n",
      "3                   1        0  \n",
      "4                   0        0  \n",
      "...               ...      ...  \n",
      "354041              0        1  \n",
      "354042              1        1  \n",
      "354043              1        1  \n",
      "354044              1        1  \n",
      "354045              1        1  \n",
      "\n",
      "[354046 rows x 17 columns]\n",
      "------------------------------\n",
      "                   placekey          city region           date_range_start  \\\n",
      "0       222-222@5q6-7qg-qxq      Longmont     CO  2018-01-01 00:00:00+00:00   \n",
      "1       222-222@5pv-yjy-92k  Merrillville     IN  2018-12-03 00:00:00+00:00   \n",
      "2       222-222@5pv-yjy-92k  Merrillville     IN  2018-04-23 00:00:00+00:00   \n",
      "3       222-222@5pv-yjy-92k  Merrillville     IN  2018-03-05 00:00:00+00:00   \n",
      "4       222-222@5pk-ns6-vvf     Bridgeton     MO  2018-11-12 00:00:00+00:00   \n",
      "...                     ...           ...    ...                        ...   \n",
      "269607  222-222@627-s86-9s5      Brooklyn     NY  2022-11-28 00:00:00+00:00   \n",
      "269608  zzw-222@8gg-szq-9cq  Wade Hampton     SC  2022-11-28 00:00:00+00:00   \n",
      "269609  222-222@5vg-9s8-68v     Pittsburg     CA  2022-04-04 00:00:00+00:00   \n",
      "269610  222-222@63j-rny-qzz        Maumee     OH  2022-02-21 00:00:00+00:00   \n",
      "269611  222-222@5vg-9s8-68v     Pittsburg     CA  2022-10-31 00:00:00+00:00   \n",
      "\n",
      "                   date_range_end  raw_visit_counts           visits_by_day  \\\n",
      "0       2018-01-08 00:00:00+00:00                47       [3,3,4,4,15,10,8]   \n",
      "1       2018-12-10 00:00:00+00:00               105    [12,14,6,6,16,26,25]   \n",
      "2       2018-04-30 00:00:00+00:00               107  [11,12,11,11,18,20,24]   \n",
      "3       2018-03-12 00:00:00+00:00               122    [8,11,8,21,12,34,28]   \n",
      "4       2018-11-19 00:00:00+00:00               176  [21,20,17,12,32,40,34]   \n",
      "...                           ...               ...                     ...   \n",
      "269607  2022-12-05 00:00:00+00:00               138    [6,4,10,17,33,34,34]   \n",
      "269608  2022-12-05 00:00:00+00:00               112   [8,11,14,18,19,23,19]   \n",
      "269609  2022-04-11 00:00:00+00:00                93     [9,7,12,5,25,16,19]   \n",
      "269610  2022-02-28 00:00:00+00:00               182  [26,15,21,16,32,49,23]   \n",
      "269611  2022-11-07 00:00:00+00:00                75      [7,14,5,8,6,20,15]   \n",
      "\n",
      "                              safegraph_brand_ids  naics_code  postal_code  \\\n",
      "0       SG_BRAND_555bc88c634b70fc1811399c472524bc      722511        80501   \n",
      "1       SG_BRAND_555bc88c634b70fc1811399c472524bc      722511        46410   \n",
      "2       SG_BRAND_555bc88c634b70fc1811399c472524bc      722511        46410   \n",
      "3       SG_BRAND_555bc88c634b70fc1811399c472524bc      722511        46410   \n",
      "4       SG_BRAND_555bc88c634b70fc1811399c472524bc      722511        63044   \n",
      "...                                           ...         ...          ...   \n",
      "269607  SG_BRAND_555bc88c634b70fc1811399c472524bc      722511        11239   \n",
      "269608  SG_BRAND_555bc88c634b70fc1811399c472524bc      722511        29615   \n",
      "269609  SG_BRAND_555bc88c634b70fc1811399c472524bc      722511        94565   \n",
      "269610  SG_BRAND_555bc88c634b70fc1811399c472524bc      722511        43537   \n",
      "269611  SG_BRAND_555bc88c634b70fc1811399c472524bc      722511        94565   \n",
      "\n",
      "             brands            day  dailyvisits dayofweek  manyvisits  \\\n",
      "0       Red Lobster  daily_visits1            3    Monday           0   \n",
      "1       Red Lobster  daily_visits1           12    Monday           0   \n",
      "2       Red Lobster  daily_visits1           11    Monday           0   \n",
      "3       Red Lobster  daily_visits1            8    Monday           0   \n",
      "4       Red Lobster  daily_visits1           21    Monday           1   \n",
      "...             ...            ...          ...       ...         ...   \n",
      "269607  Red Lobster  daily_visits7           34    Sunday           1   \n",
      "269608  Red Lobster  daily_visits7           19    Sunday           1   \n",
      "269609  Red Lobster  daily_visits7           19    Sunday           1   \n",
      "269610  Red Lobster  daily_visits7           23    Sunday           1   \n",
      "269611  Red Lobster  daily_visits7           15    Sunday           0   \n",
      "\n",
      "        core_biz_area  \n",
      "0                   1  \n",
      "1                   1  \n",
      "2                   1  \n",
      "3                   1  \n",
      "4                   0  \n",
      "...               ...  \n",
      "269607              1  \n",
      "269608              0  \n",
      "269609              1  \n",
      "269610              1  \n",
      "269611              1  \n",
      "\n",
      "[269612 rows x 16 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "focal_df = pd.read_csv(\"long_sample_csv.csv\")\n",
    "benchmark_df = pd.read_csv(\"benchmark_long.csv\")\n",
    "print(focal_df)\n",
    "print(\"------------------------------\")\n",
    "print(benchmark_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "683fc7cf-480a-40f4-a6aa-be62ec7aea86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      region           date_range_start  dayofweek  Nunits  SumDailyVisits  \\\n",
      "0         AL  2018-01-01 00:00:00+00:00     Friday       4              37   \n",
      "1         AL  2018-01-01 00:00:00+00:00     Monday       4              28   \n",
      "2         AL  2018-01-01 00:00:00+00:00   Saturday       4              94   \n",
      "3         AL  2018-01-01 00:00:00+00:00     Sunday       4              71   \n",
      "4         AL  2018-01-01 00:00:00+00:00   Thursday       4              31   \n",
      "...      ...                        ...        ...     ...             ...   \n",
      "69176     WI  2022-12-26 00:00:00+00:00   Saturday       3              48   \n",
      "69177     WI  2022-12-26 00:00:00+00:00     Sunday       3               4   \n",
      "69178     WI  2022-12-26 00:00:00+00:00   Thursday       3              47   \n",
      "69179     WI  2022-12-26 00:00:00+00:00    Tuesday       3              50   \n",
      "69180     WI  2022-12-26 00:00:00+00:00  Wednesday       3              31   \n",
      "\n",
      "       MaxDailyVisitsPerUnit  MinDailyVisitsPerUnit  MedDailyVisitsPerUnit  \n",
      "0                         19                      5                    6.5  \n",
      "1                         15                      3                    5.0  \n",
      "2                         39                     13                   21.0  \n",
      "3                         31                      8                   16.0  \n",
      "4                         11                      5                    7.5  \n",
      "...                      ...                    ...                    ...  \n",
      "69176                     32                      6                   10.0  \n",
      "69177                      4                      0                    0.0  \n",
      "69178                     26                      6                   15.0  \n",
      "69179                     29                     10                   11.0  \n",
      "69180                     19                      3                    9.0  \n",
      "\n",
      "[69181 rows x 8 columns]\n",
      "----------------\n",
      "      region           date_range_start  dayofweek  Nunits  SumDailyVisits  \\\n",
      "0         AL  2018-01-01 00:00:00+00:00     Friday       2              84   \n",
      "1         AL  2018-01-01 00:00:00+00:00     Monday       2              38   \n",
      "2         AL  2018-01-01 00:00:00+00:00   Saturday       2              93   \n",
      "3         AL  2018-01-01 00:00:00+00:00     Sunday       2              89   \n",
      "4         AL  2018-01-01 00:00:00+00:00   Thursday       2              45   \n",
      "...      ...                        ...        ...     ...             ...   \n",
      "75812     WV  2022-12-26 00:00:00+00:00   Saturday       1              70   \n",
      "75813     WV  2022-12-26 00:00:00+00:00     Sunday       1              13   \n",
      "75814     WV  2022-12-26 00:00:00+00:00   Thursday       1              59   \n",
      "75815     WV  2022-12-26 00:00:00+00:00    Tuesday       1              74   \n",
      "75816     WV  2022-12-26 00:00:00+00:00  Wednesday       1              50   \n",
      "\n",
      "       MaxDailyVisitsPerUnit  MinDailyVisitsPerUnit  MedDailyVisitsPerUnit  \n",
      "0                         50                     34                   42.0  \n",
      "1                         25                     13                   19.0  \n",
      "2                         48                     45                   46.5  \n",
      "3                         51                     38                   44.5  \n",
      "4                         24                     21                   22.5  \n",
      "...                      ...                    ...                    ...  \n",
      "75812                     70                     70                   70.0  \n",
      "75813                     13                     13                   13.0  \n",
      "75814                     59                     59                   59.0  \n",
      "75815                     74                     74                   74.0  \n",
      "75816                     50                     50                   50.0  \n",
      "\n",
      "[75817 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "BenchmarkSummary = benchmark_df.groupby(['region', 'date_range_start', 'dayofweek']).agg(\n",
    "    Nunits=('placekey', 'nunique'),\n",
    "    SumDailyVisits=('dailyvisits', 'sum'),\n",
    "    MaxDailyVisitsPerUnit=('dailyvisits', 'max'),\n",
    "    MinDailyVisitsPerUnit=('dailyvisits', 'min'),\n",
    "    MedDailyVisitsPerUnit=('dailyvisits', 'median')\n",
    ").reset_index()\n",
    "\n",
    "print(BenchmarkSummary)\n",
    "print(\"----------------\")\n",
    "\n",
    "FocalSummary = focal_df.groupby(['region', 'date_range_start', 'dayofweek']).agg(\n",
    "    Nunits=('placekey', 'nunique'),\n",
    "    SumDailyVisits=('dailyvisits', 'sum'),\n",
    "    MaxDailyVisitsPerUnit=('dailyvisits', 'max'),\n",
    "    MinDailyVisitsPerUnit=('dailyvisits', 'min'),\n",
    "    MedDailyVisitsPerUnit=('dailyvisits', 'median'),\n",
    ").reset_index()\n",
    "\n",
    "print(FocalSummary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7900416f-9fe3-4376-b73a-dc2d88bdc9f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>region</th>\n",
       "      <th>date_range_start</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>Nunits_Benchmark</th>\n",
       "      <th>SumDailyVisits_Benchmark</th>\n",
       "      <th>MaxDailyVisitsPerUnit_Benchmark</th>\n",
       "      <th>MinDailyVisitsPerUnit_Benchmark</th>\n",
       "      <th>MedDailyVisitsPerUnit_Benchmark</th>\n",
       "      <th>Nunits_Focal</th>\n",
       "      <th>SumDailyVisits_Focal</th>\n",
       "      <th>MaxDailyVisitsPerUnit_Focal</th>\n",
       "      <th>MinDailyVisitsPerUnit_Focal</th>\n",
       "      <th>MedDailyVisitsPerUnit_Focal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AL</td>\n",
       "      <td>2018-01-01 00:00:00+00:00</td>\n",
       "      <td>Friday</td>\n",
       "      <td>4</td>\n",
       "      <td>37</td>\n",
       "      <td>19</td>\n",
       "      <td>5</td>\n",
       "      <td>6.5</td>\n",
       "      <td>2</td>\n",
       "      <td>84</td>\n",
       "      <td>50</td>\n",
       "      <td>34</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AL</td>\n",
       "      <td>2018-01-01 00:00:00+00:00</td>\n",
       "      <td>Monday</td>\n",
       "      <td>4</td>\n",
       "      <td>28</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>38</td>\n",
       "      <td>25</td>\n",
       "      <td>13</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AL</td>\n",
       "      <td>2018-01-01 00:00:00+00:00</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>4</td>\n",
       "      <td>94</td>\n",
       "      <td>39</td>\n",
       "      <td>13</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2</td>\n",
       "      <td>93</td>\n",
       "      <td>48</td>\n",
       "      <td>45</td>\n",
       "      <td>46.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AL</td>\n",
       "      <td>2018-01-01 00:00:00+00:00</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>4</td>\n",
       "      <td>71</td>\n",
       "      <td>31</td>\n",
       "      <td>8</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2</td>\n",
       "      <td>89</td>\n",
       "      <td>51</td>\n",
       "      <td>38</td>\n",
       "      <td>44.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AL</td>\n",
       "      <td>2018-01-01 00:00:00+00:00</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>4</td>\n",
       "      <td>31</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>7.5</td>\n",
       "      <td>2</td>\n",
       "      <td>45</td>\n",
       "      <td>24</td>\n",
       "      <td>21</td>\n",
       "      <td>22.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62988</th>\n",
       "      <td>WI</td>\n",
       "      <td>2022-12-26 00:00:00+00:00</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>3</td>\n",
       "      <td>48</td>\n",
       "      <td>32</td>\n",
       "      <td>6</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62989</th>\n",
       "      <td>WI</td>\n",
       "      <td>2022-12-26 00:00:00+00:00</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62990</th>\n",
       "      <td>WI</td>\n",
       "      <td>2022-12-26 00:00:00+00:00</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>3</td>\n",
       "      <td>47</td>\n",
       "      <td>26</td>\n",
       "      <td>6</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62991</th>\n",
       "      <td>WI</td>\n",
       "      <td>2022-12-26 00:00:00+00:00</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>29</td>\n",
       "      <td>10</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62992</th>\n",
       "      <td>WI</td>\n",
       "      <td>2022-12-26 00:00:00+00:00</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>33</td>\n",
       "      <td>33</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62993 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      region          date_range_start  dayofweek  Nunits_Benchmark  \\\n",
       "0         AL 2018-01-01 00:00:00+00:00     Friday                 4   \n",
       "1         AL 2018-01-01 00:00:00+00:00     Monday                 4   \n",
       "2         AL 2018-01-01 00:00:00+00:00   Saturday                 4   \n",
       "3         AL 2018-01-01 00:00:00+00:00     Sunday                 4   \n",
       "4         AL 2018-01-01 00:00:00+00:00   Thursday                 4   \n",
       "...      ...                       ...        ...               ...   \n",
       "62988     WI 2022-12-26 00:00:00+00:00   Saturday                 3   \n",
       "62989     WI 2022-12-26 00:00:00+00:00     Sunday                 3   \n",
       "62990     WI 2022-12-26 00:00:00+00:00   Thursday                 3   \n",
       "62991     WI 2022-12-26 00:00:00+00:00    Tuesday                 3   \n",
       "62992     WI 2022-12-26 00:00:00+00:00  Wednesday                 3   \n",
       "\n",
       "       SumDailyVisits_Benchmark  MaxDailyVisitsPerUnit_Benchmark  \\\n",
       "0                            37                               19   \n",
       "1                            28                               15   \n",
       "2                            94                               39   \n",
       "3                            71                               31   \n",
       "4                            31                               11   \n",
       "...                         ...                              ...   \n",
       "62988                        48                               32   \n",
       "62989                         4                                4   \n",
       "62990                        47                               26   \n",
       "62991                        50                               29   \n",
       "62992                        31                               19   \n",
       "\n",
       "       MinDailyVisitsPerUnit_Benchmark  MedDailyVisitsPerUnit_Benchmark  \\\n",
       "0                                    5                              6.5   \n",
       "1                                    3                              5.0   \n",
       "2                                   13                             21.0   \n",
       "3                                    8                             16.0   \n",
       "4                                    5                              7.5   \n",
       "...                                ...                              ...   \n",
       "62988                                6                             10.0   \n",
       "62989                                0                              0.0   \n",
       "62990                                6                             15.0   \n",
       "62991                               10                             11.0   \n",
       "62992                                3                              9.0   \n",
       "\n",
       "       Nunits_Focal  SumDailyVisits_Focal  MaxDailyVisitsPerUnit_Focal  \\\n",
       "0                 2                    84                           50   \n",
       "1                 2                    38                           25   \n",
       "2                 2                    93                           48   \n",
       "3                 2                    89                           51   \n",
       "4                 2                    45                           24   \n",
       "...             ...                   ...                          ...   \n",
       "62988             1                    38                           38   \n",
       "62989             1                     3                            3   \n",
       "62990             1                    19                           19   \n",
       "62991             1                    27                           27   \n",
       "62992             1                    33                           33   \n",
       "\n",
       "       MinDailyVisitsPerUnit_Focal  MedDailyVisitsPerUnit_Focal  \n",
       "0                               34                         42.0  \n",
       "1                               13                         19.0  \n",
       "2                               45                         46.5  \n",
       "3                               38                         44.5  \n",
       "4                               21                         22.5  \n",
       "...                            ...                          ...  \n",
       "62988                           38                         38.0  \n",
       "62989                            3                          3.0  \n",
       "62990                           19                         19.0  \n",
       "62991                           27                         27.0  \n",
       "62992                           33                         33.0  \n",
       "\n",
       "[62993 rows x 13 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#merging into merged summary\n",
    "\n",
    "BenchmarkSummary['date_range_start'] = pd.to_datetime(BenchmarkSummary['date_range_start'])\n",
    "FocalSummary['date_range_start'] = pd.to_datetime(FocalSummary['date_range_start'])\n",
    "\n",
    "MergedSummary = pd.merge(BenchmarkSummary, FocalSummary, on=['region', 'date_range_start', 'dayofweek'], suffixes=('_Benchmark', '_Focal'), how='inner')\n",
    "MergedSummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a1ef1867-f3f0-4c25-ae60-10ac1110a962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   region  Nunits_Focal  Nunits_Benchmark\n",
      "0      AL          3654              7294\n",
      "1      AZ         10626              7273\n",
      "2      CA         42784             21924\n",
      "3      CO          7308             10962\n",
      "4      CT          1827              1827\n",
      "5      FL         20440             17836\n",
      "6      GA         18263              7308\n",
      "7      ID          1827              1827\n",
      "8      IL          9100             12677\n",
      "9      IN          9135              9128\n",
      "10     KS          1827              1827\n",
      "11     KY          5481              5460\n",
      "12     MD          7308             10773\n",
      "13     MI          7308             14525\n",
      "14     MN          7308              3654\n",
      "15     MO         12789              3654\n",
      "16     MS          1827              3647\n",
      "17     NC          9135              9128\n",
      "18     ND          3654              1827\n",
      "19     NJ          9058              3654\n",
      "20     NM          1827              5481\n",
      "21     NV          7266              3654\n",
      "22     NY          8169              9135\n",
      "23     OH         16240             16415\n",
      "24     OK         11774              1827\n",
      "25     OR          3535              1827\n",
      "26     PA         10962              7308\n",
      "27     SC          8841              5460\n",
      "28     SD          1827              1827\n",
      "29     TN         10962             10962\n",
      "30     TX         34510             18270\n",
      "31     UT          5432              1827\n",
      "32     VA          5481             11270\n",
      "33     WA          9135              7308\n",
      "34     WI           875              2625\n"
     ]
    }
   ],
   "source": [
    "# Summing up the number of units for Focal and Benchmark by state\n",
    "state_unit_counts = MergedSummary.groupby('region')[['Nunits_Focal', 'Nunits_Benchmark']].sum().reset_index()\n",
    "\n",
    "print(state_unit_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b64bb8-b0f5-4e31-9ae6-4876619dddb0",
   "metadata": {},
   "source": [
    "Question 1b (2 points): Download the data for Census demographics. Merge your MergedSummary data with the demographic data by the variable \"State\".\r\n",
    "\r\n",
    "How many observations do you have in MergedSummary? How many observations do you have in the Census demographic data? How many observations are matched between the two after the merge? How many observations are in MergedSummary but not the demographic data? How many observations are in the demographic data but not in MergedSummary? What explains the imperfect match?\r\n",
    "\r\n",
    "Keep only the matched observation for the rest of the project, let us call the resulting dataset as the final analysis sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "99f8f6c7-20b5-4266-b132-1fc8d38fbbbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations: in MS 62993\n",
      "Number of observations in Census Data 52\n"
     ]
    }
   ],
   "source": [
    "census_data =  pd.read_csv(\"census_state_2016.csv\")\n",
    "census_data\n",
    "\n",
    "#number of observations in MergedSummary\n",
    "number_of_observations_MS = len(MergedSummary)\n",
    "print(\"Number of observations: in MS\", number_of_observations_MS)\n",
    "\n",
    "#number of observations in census demographics\n",
    "number_of_observations_CD = len(census_data)\n",
    "print(\"Number of observations in Census Data\" , number_of_observations_CD)\n",
    "#print(census_data.columns)\n",
    "\n",
    "MergedSummary.rename(columns={'region': 'state'}, inplace=True)\n",
    "MergedSummary.rename(columns={'state': 'state_abbr'}, inplace=True)\n",
    "#print(MergedSummary.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5c0d23e1-abea-44c4-adde-25733dc4a372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matched observations: 62993\n",
      "Observations in MergedSummary not in Census Data: 0\n",
      "Observations in Census Data not in MergedSummary: 17\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state_abbr</th>\n",
       "      <th>date_range_start</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>Nunits_Benchmark</th>\n",
       "      <th>SumDailyVisits_Benchmark</th>\n",
       "      <th>MaxDailyVisitsPerUnit_Benchmark</th>\n",
       "      <th>MinDailyVisitsPerUnit_Benchmark</th>\n",
       "      <th>MedDailyVisitsPerUnit_Benchmark</th>\n",
       "      <th>Nunits_Focal</th>\n",
       "      <th>SumDailyVisits_Focal</th>\n",
       "      <th>...</th>\n",
       "      <th>black</th>\n",
       "      <th>indian_alaska</th>\n",
       "      <th>asian</th>\n",
       "      <th>hawaiian_pacific</th>\n",
       "      <th>other_races</th>\n",
       "      <th>hispanic_latino</th>\n",
       "      <th>housing_units</th>\n",
       "      <th>citizen</th>\n",
       "      <th>citizen_male</th>\n",
       "      <th>citizen_female</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AL</td>\n",
       "      <td>2018-01-01 00:00:00+00:00</td>\n",
       "      <td>Friday</td>\n",
       "      <td>4.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1338854</td>\n",
       "      <td>56661</td>\n",
       "      <td>85704</td>\n",
       "      <td>3769</td>\n",
       "      <td>77234</td>\n",
       "      <td>199686</td>\n",
       "      <td>2230180</td>\n",
       "      <td>3670229</td>\n",
       "      <td>1742667</td>\n",
       "      <td>1927562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AL</td>\n",
       "      <td>2018-01-01 00:00:00+00:00</td>\n",
       "      <td>Monday</td>\n",
       "      <td>4.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1338854</td>\n",
       "      <td>56661</td>\n",
       "      <td>85704</td>\n",
       "      <td>3769</td>\n",
       "      <td>77234</td>\n",
       "      <td>199686</td>\n",
       "      <td>2230180</td>\n",
       "      <td>3670229</td>\n",
       "      <td>1742667</td>\n",
       "      <td>1927562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AL</td>\n",
       "      <td>2018-01-01 00:00:00+00:00</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>4.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1338854</td>\n",
       "      <td>56661</td>\n",
       "      <td>85704</td>\n",
       "      <td>3769</td>\n",
       "      <td>77234</td>\n",
       "      <td>199686</td>\n",
       "      <td>2230180</td>\n",
       "      <td>3670229</td>\n",
       "      <td>1742667</td>\n",
       "      <td>1927562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AL</td>\n",
       "      <td>2018-01-01 00:00:00+00:00</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>4.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1338854</td>\n",
       "      <td>56661</td>\n",
       "      <td>85704</td>\n",
       "      <td>3769</td>\n",
       "      <td>77234</td>\n",
       "      <td>199686</td>\n",
       "      <td>2230180</td>\n",
       "      <td>3670229</td>\n",
       "      <td>1742667</td>\n",
       "      <td>1927562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AL</td>\n",
       "      <td>2018-01-01 00:00:00+00:00</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>4.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1338854</td>\n",
       "      <td>56661</td>\n",
       "      <td>85704</td>\n",
       "      <td>3769</td>\n",
       "      <td>77234</td>\n",
       "      <td>199686</td>\n",
       "      <td>2230180</td>\n",
       "      <td>3670229</td>\n",
       "      <td>1742667</td>\n",
       "      <td>1927562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62988</th>\n",
       "      <td>WI</td>\n",
       "      <td>2022-12-26 00:00:00+00:00</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>3.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>...</td>\n",
       "      <td>428940</td>\n",
       "      <td>93580</td>\n",
       "      <td>186501</td>\n",
       "      <td>6098</td>\n",
       "      <td>144989</td>\n",
       "      <td>387015</td>\n",
       "      <td>2668443</td>\n",
       "      <td>4351720</td>\n",
       "      <td>2138987</td>\n",
       "      <td>2212733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62989</th>\n",
       "      <td>WI</td>\n",
       "      <td>2022-12-26 00:00:00+00:00</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>428940</td>\n",
       "      <td>93580</td>\n",
       "      <td>186501</td>\n",
       "      <td>6098</td>\n",
       "      <td>144989</td>\n",
       "      <td>387015</td>\n",
       "      <td>2668443</td>\n",
       "      <td>4351720</td>\n",
       "      <td>2138987</td>\n",
       "      <td>2212733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62990</th>\n",
       "      <td>WI</td>\n",
       "      <td>2022-12-26 00:00:00+00:00</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>3.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>...</td>\n",
       "      <td>428940</td>\n",
       "      <td>93580</td>\n",
       "      <td>186501</td>\n",
       "      <td>6098</td>\n",
       "      <td>144989</td>\n",
       "      <td>387015</td>\n",
       "      <td>2668443</td>\n",
       "      <td>4351720</td>\n",
       "      <td>2138987</td>\n",
       "      <td>2212733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62991</th>\n",
       "      <td>WI</td>\n",
       "      <td>2022-12-26 00:00:00+00:00</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>3.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>...</td>\n",
       "      <td>428940</td>\n",
       "      <td>93580</td>\n",
       "      <td>186501</td>\n",
       "      <td>6098</td>\n",
       "      <td>144989</td>\n",
       "      <td>387015</td>\n",
       "      <td>2668443</td>\n",
       "      <td>4351720</td>\n",
       "      <td>2138987</td>\n",
       "      <td>2212733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62992</th>\n",
       "      <td>WI</td>\n",
       "      <td>2022-12-26 00:00:00+00:00</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>3.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>...</td>\n",
       "      <td>428940</td>\n",
       "      <td>93580</td>\n",
       "      <td>186501</td>\n",
       "      <td>6098</td>\n",
       "      <td>144989</td>\n",
       "      <td>387015</td>\n",
       "      <td>2668443</td>\n",
       "      <td>4351720</td>\n",
       "      <td>2138987</td>\n",
       "      <td>2212733</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62993 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      state_abbr          date_range_start  dayofweek  Nunits_Benchmark  \\\n",
       "0             AL 2018-01-01 00:00:00+00:00     Friday               4.0   \n",
       "1             AL 2018-01-01 00:00:00+00:00     Monday               4.0   \n",
       "2             AL 2018-01-01 00:00:00+00:00   Saturday               4.0   \n",
       "3             AL 2018-01-01 00:00:00+00:00     Sunday               4.0   \n",
       "4             AL 2018-01-01 00:00:00+00:00   Thursday               4.0   \n",
       "...          ...                       ...        ...               ...   \n",
       "62988         WI 2022-12-26 00:00:00+00:00   Saturday               3.0   \n",
       "62989         WI 2022-12-26 00:00:00+00:00     Sunday               3.0   \n",
       "62990         WI 2022-12-26 00:00:00+00:00   Thursday               3.0   \n",
       "62991         WI 2022-12-26 00:00:00+00:00    Tuesday               3.0   \n",
       "62992         WI 2022-12-26 00:00:00+00:00  Wednesday               3.0   \n",
       "\n",
       "       SumDailyVisits_Benchmark  MaxDailyVisitsPerUnit_Benchmark  \\\n",
       "0                          37.0                             19.0   \n",
       "1                          28.0                             15.0   \n",
       "2                          94.0                             39.0   \n",
       "3                          71.0                             31.0   \n",
       "4                          31.0                             11.0   \n",
       "...                         ...                              ...   \n",
       "62988                      48.0                             32.0   \n",
       "62989                       4.0                              4.0   \n",
       "62990                      47.0                             26.0   \n",
       "62991                      50.0                             29.0   \n",
       "62992                      31.0                             19.0   \n",
       "\n",
       "       MinDailyVisitsPerUnit_Benchmark  MedDailyVisitsPerUnit_Benchmark  \\\n",
       "0                                  5.0                              6.5   \n",
       "1                                  3.0                              5.0   \n",
       "2                                 13.0                             21.0   \n",
       "3                                  8.0                             16.0   \n",
       "4                                  5.0                              7.5   \n",
       "...                                ...                              ...   \n",
       "62988                              6.0                             10.0   \n",
       "62989                              0.0                              0.0   \n",
       "62990                              6.0                             15.0   \n",
       "62991                             10.0                             11.0   \n",
       "62992                              3.0                              9.0   \n",
       "\n",
       "       Nunits_Focal  SumDailyVisits_Focal  ...    black  indian_alaska  \\\n",
       "0               2.0                  84.0  ...  1338854          56661   \n",
       "1               2.0                  38.0  ...  1338854          56661   \n",
       "2               2.0                  93.0  ...  1338854          56661   \n",
       "3               2.0                  89.0  ...  1338854          56661   \n",
       "4               2.0                  45.0  ...  1338854          56661   \n",
       "...             ...                   ...  ...      ...            ...   \n",
       "62988           1.0                  38.0  ...   428940          93580   \n",
       "62989           1.0                   3.0  ...   428940          93580   \n",
       "62990           1.0                  19.0  ...   428940          93580   \n",
       "62991           1.0                  27.0  ...   428940          93580   \n",
       "62992           1.0                  33.0  ...   428940          93580   \n",
       "\n",
       "        asian hawaiian_pacific  other_races  hispanic_latino  housing_units  \\\n",
       "0       85704             3769        77234           199686        2230180   \n",
       "1       85704             3769        77234           199686        2230180   \n",
       "2       85704             3769        77234           199686        2230180   \n",
       "3       85704             3769        77234           199686        2230180   \n",
       "4       85704             3769        77234           199686        2230180   \n",
       "...       ...              ...          ...              ...            ...   \n",
       "62988  186501             6098       144989           387015        2668443   \n",
       "62989  186501             6098       144989           387015        2668443   \n",
       "62990  186501             6098       144989           387015        2668443   \n",
       "62991  186501             6098       144989           387015        2668443   \n",
       "62992  186501             6098       144989           387015        2668443   \n",
       "\n",
       "       citizen  citizen_male  citizen_female  \n",
       "0      3670229       1742667         1927562  \n",
       "1      3670229       1742667         1927562  \n",
       "2      3670229       1742667         1927562  \n",
       "3      3670229       1742667         1927562  \n",
       "4      3670229       1742667         1927562  \n",
       "...        ...           ...             ...  \n",
       "62988  4351720       2138987         2212733  \n",
       "62989  4351720       2138987         2212733  \n",
       "62990  4351720       2138987         2212733  \n",
       "62991  4351720       2138987         2212733  \n",
       "62992  4351720       2138987         2212733  \n",
       "\n",
       "[62993 rows x 35 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_data = pd.merge(MergedSummary, census_data, on='state_abbr', how='outer', indicator=True)\n",
    "matched_observations = len(merged_data[merged_data['_merge'] == 'both'])\n",
    "ms_not_in_cd = len(merged_data[merged_data['_merge'] == 'left_only'])\n",
    "cd_not_in_ms = len(merged_data[merged_data['_merge'] == 'right_only'])\n",
    "\n",
    "print(\"Number of matched observations:\", matched_observations)\n",
    "print(\"Observations in MergedSummary not in Census Data:\", ms_not_in_cd)\n",
    "print(\"Observations in Census Data not in MergedSummary:\", cd_not_in_ms)\n",
    "\n",
    "final_analysis_sample = merged_data[merged_data['_merge'] == 'both'].drop(columns='_merge')\n",
    "final_analysis_sample \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39aa80d-0bf8-4b46-8e99-f38487dd093f",
   "metadata": {},
   "source": [
    "Observations in MergedSummary not in Census Data: 0\n",
    "Observations in Census Data not in MergedSummary: 17\n",
    "The reason for the imperfect match in the number of observations in Census Data not in MergedSummary is because of the additional states/terrortories in census data (17 states to be exact - Puerto Rico, District of Columbia, etc. that are not included MergedSummary). MergedSummary only accounts for 35 states while census data accounts for 52, hence the imperfect match while merging. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc161a3-c9ed-4c56-98c8-9b616bd35c2f",
   "metadata": {},
   "source": [
    "Question 1c (2 points): Now using the final analysis sample, define your dependent variable as\n",
    "\n",
    "                  Y = [Ln(SumDailyVisits+1) of your focal chain - Ln(SumDailyVisits+1) of your benchmark chain].\n",
    "If you believe you can have a better variable describing the performance of your focal chain relative to your benchmark chain, you are more than welcome to use your own dependent variable, as long as you explain why your choice is better.\n",
    "\n",
    "Define the list of your potential right-hand variables, which should include day of week indicators (Monday, Tuesday …, Sunday), some demographics, some state attributes (e.g. whether the state is red or blue as you have defined in Project 3). Feel free to add new variables to this list, as long as you articulate a reason for their relevance.\n",
    "\n",
    "Summarize all these variables in a table with the number of observations, mean, median, standard deviation, minimum and maximum. It is ok to use multiple tables if that is more convenient for you. If some variables are categorical and thus cannot appear in this summary table, create separate table(s) to describe the frequency of each value in such categorical variable(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "89d8e2fe-5c03-43b2-8ac4-12500f1f1e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(final_analysis_sample.columns)\n",
    "final_analysis_sample['Y'] = np.log(final_analysis_sample['SumDailyVisits_Focal'] + 1) - np.log(final_analysis_sample['SumDailyVisits_Benchmark'] + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "375bc375-383f-423b-a948-e357aaea9a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day of Week Distribution:\n",
      " dayofweek\n",
      "Friday       8999\n",
      "Monday       8999\n",
      "Saturday     8999\n",
      "Sunday       8999\n",
      "Thursday     8999\n",
      "Tuesday      8999\n",
      "Wednesday    8999\n",
      "Name: count, dtype: int64\n",
      "--------------------------------------------------\n",
      "summarizing demographic data\n",
      "       SumDailyVisits_Focal  SumDailyVisits_Benchmark  total_population  \\\n",
      "count          62993.000000              62993.000000      6.299300e+04   \n",
      "mean             172.398251                 64.008715      8.381432e+06   \n",
      "std              192.197485                 59.144515      7.854247e+06   \n",
      "min                0.000000                  0.000000      7.579530e+05   \n",
      "25%               57.000000                 20.000000      3.576452e+06   \n",
      "50%              113.000000                 46.000000      6.016447e+06   \n",
      "75%              211.000000                 90.000000      1.014679e+07   \n",
      "max             2005.000000                723.000000      3.925002e+07   \n",
      "\n",
      "         median_age  housing_units    age_over65  \n",
      "count  62993.000000   6.299300e+04  6.299300e+04  \n",
      "mean      37.842527   3.493846e+06  1.268807e+06  \n",
      "std        1.990844   2.999449e+06  1.134373e+06  \n",
      "min       30.700000   3.685450e+05  1.098510e+05  \n",
      "25%       36.700000   1.499145e+06  5.766400e+05  \n",
      "50%       37.900000   2.668443e+06  9.272610e+05  \n",
      "75%       39.000000   4.540697e+06  1.568591e+06  \n",
      "max       42.100000   1.406138e+07  5.348439e+06  \n",
      "--------------------------------------------------\n",
      "State Color Distribution:\n",
      " BlueState\n",
      "0    37415\n",
      "1    25578\n",
      "Name: count, dtype: int64\n",
      "-----------------------------------------------------------------\n",
      "DESCRIPTIVE STATS FOR ALL THESE VARIABLES\n",
      "       Nunits_Benchmark  SumDailyVisits_Benchmark  \\\n",
      "count      62993.000000              62993.000000   \n",
      "mean           4.149683                 64.008715   \n",
      "std            2.934795                 59.144515   \n",
      "min            1.000000                  0.000000   \n",
      "25%            2.000000                 20.000000   \n",
      "50%            4.000000                 46.000000   \n",
      "75%            6.000000                 90.000000   \n",
      "max           12.000000                723.000000   \n",
      "\n",
      "       MaxDailyVisitsPerUnit_Benchmark  MinDailyVisitsPerUnit_Benchmark  \\\n",
      "count                     62993.000000                     62993.000000   \n",
      "mean                         23.021479                         9.472386   \n",
      "std                          14.843607                         8.019008   \n",
      "min                           0.000000                         0.000000   \n",
      "25%                          13.000000                         4.000000   \n",
      "50%                          20.000000                         8.000000   \n",
      "75%                          29.000000                        13.000000   \n",
      "max                         252.000000                        82.000000   \n",
      "\n",
      "       MedDailyVisitsPerUnit_Benchmark  Nunits_Focal  SumDailyVisits_Focal  \\\n",
      "count                     62993.000000  62993.000000          62993.000000   \n",
      "mean                         14.840982      5.198911            172.398251   \n",
      "std                           8.749836      4.719692            192.197485   \n",
      "min                           0.000000      1.000000              0.000000   \n",
      "25%                           9.000000      2.000000             57.000000   \n",
      "50%                          13.000000      4.000000            113.000000   \n",
      "75%                          19.000000      6.000000            211.000000   \n",
      "max                          89.500000     24.000000           2005.000000   \n",
      "\n",
      "       MaxDailyVisitsPerUnit_Focal  MinDailyVisitsPerUnit_Focal  \\\n",
      "count                 62993.000000                 62993.000000   \n",
      "mean                     46.310669                    21.644961   \n",
      "std                      26.007340                    18.276924   \n",
      "min                       0.000000                     0.000000   \n",
      "25%                      28.000000                    10.000000   \n",
      "50%                      41.000000                    18.000000   \n",
      "75%                      59.000000                    29.000000   \n",
      "max                     269.000000                   189.000000   \n",
      "\n",
      "       MedDailyVisitsPerUnit_Focal  ...         asian  hawaiian_pacific  \\\n",
      "count                 62993.000000  ...  6.299300e+04      62993.000000   \n",
      "mean                     33.136087  ...  5.480825e+05      26842.757973   \n",
      "std                      18.785851  ...  1.094663e+06      53703.920281   \n",
      "min                       0.000000  ...  1.347000e+04       1708.000000   \n",
      "25%                      21.000000  ...  1.007950e+05       5946.000000   \n",
      "50%                      29.500000  ...  2.358010e+05      12053.000000   \n",
      "75%                      41.000000  ...  4.937810e+05      30833.000000   \n",
      "max                     194.000000  ...  6.432756e+06     319337.000000   \n",
      "\n",
      "        other_races  hispanic_latino  housing_units       citizen  \\\n",
      "count  6.299300e+04     6.299300e+04   6.299300e+04  6.299300e+04   \n",
      "mean   4.981913e+05     1.591438e+06   3.493846e+06  5.901276e+06   \n",
      "std    1.054275e+06     3.080295e+06   2.999449e+06  5.141045e+06   \n",
      "min    7.531000e+03     2.643400e+04   3.685450e+05  5.706970e+05   \n",
      "25%    8.835400e+04     2.884600e+05   1.499145e+06  2.602785e+06   \n",
      "50%    1.804600e+05     5.225680e+05   2.668443e+06  4.351720e+06   \n",
      "75%    3.413500e+05     1.009876e+06   4.540697e+06  7.418127e+06   \n",
      "max    6.074661e+06     1.528078e+07   1.406138e+07  2.519780e+07   \n",
      "\n",
      "       citizen_male  citizen_female             Y     BlueState  \n",
      "count  6.299300e+04    6.299300e+04  62993.000000  62993.000000  \n",
      "mean   2.857465e+06    3.043811e+06      0.944407      0.406045  \n",
      "std    2.495924e+06    2.645882e+06      0.818774      0.491097  \n",
      "min    2.925690e+05    2.781280e+05     -4.574711      0.000000  \n",
      "25%    1.248029e+06    1.354756e+06      0.427938      0.000000  \n",
      "50%    2.138987e+06    2.239079e+06      0.900068      0.000000  \n",
      "75%    3.529077e+06    3.837312e+06      1.367702      1.000000  \n",
      "max    1.233967e+07    1.285813e+07      5.505332      1.000000  \n",
      "\n",
      "[8 rows x 33 columns]\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "#define list of potential right hand variables\n",
    "#day of the week indicators\n",
    "day_of_week_counts = final_analysis_sample['dayofweek'].value_counts()\n",
    "print(\"Day of Week Distribution:\\n\", day_of_week_counts)\n",
    "print(\"--------------------------------------------------\")\n",
    "\n",
    "#some demographics\n",
    "continuous_vars = final_analysis_sample[['SumDailyVisits_Focal', 'SumDailyVisits_Benchmark', 'total_population', 'median_age', 'housing_units', 'age_over65']]\n",
    "# Generating descriptive statistics\n",
    "continuous_summary = continuous_vars.describe()\n",
    "print(\"summarizing demographic data\")\n",
    "print(continuous_summary)\n",
    "print(\"--------------------------------------------------\")\n",
    "\n",
    "#some state attributes whether state is blue or red\n",
    "Red_Blue_States = pd.read_csv(\"RedBlueStates.csv\")\n",
    "\n",
    "Red_Blue_States['BlueState'] = (Red_Blue_States['Color'] == 'Blue').astype(int)\n",
    "\n",
    "state_color_mapping = Red_Blue_States.set_index('State')['BlueState'].to_dict()\n",
    "\n",
    "final_analysis_sample['BlueState'] = final_analysis_sample['state_abbr'].map(state_color_mapping)\n",
    "\n",
    "if 'BlueState' in final_analysis_sample.columns:\n",
    "    blue_state_counts = final_analysis_sample['BlueState'].value_counts()\n",
    "     \n",
    "    print(\"State Color Distribution:\\n\", blue_state_counts)\n",
    "    \n",
    "else:\n",
    "    \n",
    "    print(\"BlueState information not available.\")\n",
    "\n",
    "print(\"-----------------------------------------------------------------\")\n",
    "print(\"DESCRIPTIVE STATS FOR ALL THESE VARIABLES\")\n",
    "descriptive_stats = final_analysis_sample.describe()\n",
    "print(descriptive_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416eabd5-ed6a-4d6a-bd32-30163bc61ebe",
   "metadata": {},
   "source": [
    "Part 2: Linear regression, LASSO and Ridge\r",
    " \r\n",
    "\r\n",
    "Question 2a (2 points): Choose a list of variables to predict your Y. The list you choose could be a subset of the potential right-hand variables you have identified in Question 1c. I will refer to these variables as “featur.\r\n",
    "\r\n",
    " \r\n",
    "\r\n",
    "Use the Python command “train_test_split()” to separate your final analysis sample into a training sample and a testsaple.\r\n",
    "\r\n",
    " \r\n",
    "\r\n",
    "Use function MinMaxScalar() to standardize all the “features” you choose, in both the training and te samples.\r\n",
    "\r\n",
    " \r\n",
    "\r\n",
    "How many observations are there in the training sample? How many in the test sample? Choose at least two variables to comment on the difference between the two samples. For example, are they different in the months covered? Are they different in certain demographics? Feel free to use other variables for this comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "71ef8fd8-e997-45c7-8837-f723007215bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations in the training sample: 50394\n",
      "Number of observations in the test sample: 12599\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define features and target variable\n",
    "#need to onehotencode days of the week because it is a string\n",
    "day_dummies = pd.get_dummies(final_analysis_sample['dayofweek'], prefix='Day')\n",
    "final_analysis_sample_encoded = pd.concat([final_analysis_sample, day_dummies], axis=1)\n",
    "\n",
    "X = final_analysis_sample_encoded[['SumDailyVisits_Focal', 'SumDailyVisits_Benchmark', 'total_population', 'median_age', 'housing_units', 'age_over65', 'BlueState'] + list(day_dummies.columns)]\n",
    "Y = final_analysis_sample_encoded['Y']\n",
    "\n",
    "X = X.astype(float)\n",
    "# Splitting the data into train and test sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "Y_train = Y_train.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "Y_test = Y_test.reset_index(drop=True)\n",
    "\n",
    "print(f\"Number of observations in the training sample: {X_train.shape[0]}\")\n",
    "print(f\"Number of observations in the test sample: {X_test.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "100becd6-d34d-47d8-94b1-26c15fa9ba95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, message=\"is_sparse is deprecated*\")\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "#print(X_train.dtypes)\n",
    "\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd33c2f-c2c4-4b28-848e-abc31d93989a",
   "metadata": {},
   "source": [
    "number of observations in training sample is 50394 and number of obs in the test sample is 12599. choose atleast two variables to comment on the difference between the two samples..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "036a68ae-c0a3-4850-b809-8e5d629279fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Descriptive Statistics:\n",
      "        total_population    median_age\n",
      "count      50394.000000  50394.000000\n",
      "mean           0.197933      0.626522\n",
      "std            0.204540      0.174493\n",
      "min            0.000000      0.000000\n",
      "25%            0.073223      0.526316\n",
      "50%            0.136612      0.631579\n",
      "75%            0.243916      0.728070\n",
      "max            1.000000      1.000000\n",
      "Test Data Descriptive Statistics:\n",
      "        total_population    median_age\n",
      "count      12599.000000  12599.000000\n",
      "mean           0.198535      0.626601\n",
      "std            0.202079      0.175213\n",
      "min            0.000000      0.000000\n",
      "25%            0.073223      0.526316\n",
      "50%            0.136612      0.631579\n",
      "75%            0.243916      0.728070\n",
      "max            1.000000      1.000000\n"
     ]
    }
   ],
   "source": [
    "variables_to_compare = ['total_population', 'median_age']\n",
    "\n",
    "# Descriptive statistics for the training data\n",
    "train_stats = X_train_scaled_df[variables_to_compare].describe()\n",
    "print(\"Training Data Descriptive Statistics:\\n\", train_stats)\n",
    "\n",
    "# Descriptive statistics for the test data\n",
    "test_stats = X_test_scaled_df[variables_to_compare].describe()\n",
    "print(\"Test Data Descriptive Statistics:\\n\", test_stats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b56094-0c3a-4200-b9ad-14d3db89dc6a",
   "metadata": {},
   "source": [
    "The two variables total_population, and median_age show a very similar mean in both the test and training samples meaning that both demographic variables are evenly split and well represented in both the training and test sets. This indicates that there is not any skew or bias bewteen the two samples, which is crucial in a machine learning model -> the close split and similarity in distribution show that\n",
    "random split of the data was done correctly -> any patterns that were learned by the model in the training set was reflected on the test data sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daafbe7e-6d69-4bd2-89d0-9fca83054bb4",
   "metadata": {},
   "source": [
    "2b) Using Y and the standardized features in Question 2a, run a linear regression in your training sample. Report the coefficient estimates and R-squares.\r",
    " \r\n",
    "\r\n",
    "What “features” have statistically significant coefficients (with p-value<0.05)? Among these significant “features”, which has a positive coefficient and which has a negative coefficient? Comment on whether their signs and magnitude confirm your pr\n",
    "\r\n",
    "\r\n",
    " \r\n",
    "\r\n",
    "If you extend the model to predict Y in the test sample, what is the R-squares in the testmple?\r\n",
    "\r\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6ee6e5be-1cf3-4dc0-ade7-50e72678f017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      Y   R-squared:                       0.573\n",
      "Model:                            OLS   Adj. R-squared:                  0.573\n",
      "Method:                 Least Squares   F-statistic:                     5211.\n",
      "Date:                Sat, 20 Apr 2024   Prob (F-statistic):               0.00\n",
      "Time:                        10:55:50   Log-Likelihood:                -40095.\n",
      "No. Observations:               50394   AIC:                         8.022e+04\n",
      "Df Residuals:                   50380   BIC:                         8.034e+04\n",
      "Df Model:                          13                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "============================================================================================\n",
      "                               coef    std err          t      P>|t|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------------------\n",
      "const                        1.4030      0.010    138.336      0.000       1.383       1.423\n",
      "SumDailyVisits_Focal         9.1600      0.044    205.960      0.000       9.073       9.247\n",
      "SumDailyVisits_Benchmark    -9.0217      0.048   -189.215      0.000      -9.115      -8.928\n",
      "total_population            -2.4332      0.141    -17.251      0.000      -2.710      -2.157\n",
      "median_age                  -0.8692      0.021    -41.961      0.000      -0.910      -0.829\n",
      "housing_units               -4.5168      0.164    -27.593      0.000      -4.838      -4.196\n",
      "age_over65                   6.4767      0.093     69.351      0.000       6.294       6.660\n",
      "BlueState                   -0.0561      0.006     -9.679      0.000      -0.067      -0.045\n",
      "Day_Friday                   0.2029      0.006     32.863      0.000       0.191       0.215\n",
      "Day_Monday                   0.1646      0.006     27.161      0.000       0.153       0.177\n",
      "Day_Saturday                 0.2290      0.006     36.099      0.000       0.217       0.241\n",
      "Day_Sunday                   0.2327      0.006     38.112      0.000       0.221       0.245\n",
      "Day_Thursday                 0.1821      0.006     30.134      0.000       0.170       0.194\n",
      "Day_Tuesday                  0.1955      0.006     32.095      0.000       0.184       0.207\n",
      "Day_Wednesday                0.1962      0.006     32.246      0.000       0.184       0.208\n",
      "==============================================================================\n",
      "Omnibus:                     4973.548   Durbin-Watson:                   2.004\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            36027.941\n",
      "Skew:                          -0.171   Prob(JB):                         0.00\n",
      "Kurtosis:                       7.128   Cond. No.                     3.26e+15\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is 9.05e-27. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "X_train_sm = sm.add_constant(X_train_scaled_df)\n",
    "\n",
    "model = sm.OLS(Y_train, X_train_sm).fit()\n",
    "\n",
    "# Print the summary of the regression\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfced78c-0360-4672-b184-aac13fd9bed5",
   "metadata": {},
   "source": [
    "R^2 value is 0.573 meaning that approx 57.3% of variance in Y is due to model's X inputs. Statistically signficant features p < 0.05 are SumDailyVisits_Benchmark, total_population,median_age, housing_units, and BlueState, their signs are negative, and confirm that they are statsitcally signficant. implies that as benchmark chain visits increase, performance Y decreases which matches how Y is defined (Focal - Benchmark). It also shows a negative coefficent for total population which could imply that in areas with larger populations there could be increased variety of different consumer preferences, hence focal chain has a possibility of not performing as well as the benchmark age. there is a similar negative coefficent in median_age, housing_units, and blue state show that focal chain is less poopular among certain demographics than benchmark chain, but other confounding variables can influence this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "011fba44-3a59-4b8e-8a72-302cb8f3158d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared for the test sample: 0.5704099441986961\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "X_test_sm = sm.add_constant(X_test_scaled_df)\n",
    "\n",
    "# Predict Y using the test data\n",
    "Y_pred_test = model.predict(X_test_sm)\n",
    "\n",
    "# Calculate the R-squared for the test sample\n",
    "r_squared_test = r2_score(Y_test, Y_pred_test)\n",
    "print(f\"R-squared for the test sample: {r_squared_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3f879e-ad36-44b1-b5df-cf44255a089b",
   "metadata": {},
   "source": [
    "Question 2c (3 points): Using Y and the standardized features in Question 2a, run LASSO in your training sample.\r",
    " \r\n",
    "\r\n",
    "Try at least three different values for the penalty parameter. For each value you have tried, report the coefficient estimates and R-squares; also extend the model to predict Y in the test sample, and report the R-squares in the test samp\r\n",
    "m\n",
    " \r\n",
    "\r\n",
    "Among the penalty parameters you have tried, which one is most appropriate? Explain how you arrive at this ice.\r\n",
    "\r\n",
    " \r\n",
    "\r\n",
    "In the LASSO model with your preferred choice of the penalty parameter, what “features” have zero (or very close to zero) coefficients? What and how many “features” have non-zero coefficients? Comment on whether their signs and magnitude confirm your prior. What is the R-squares when you apply this model to the test sample? Is it higher or lower than what you find in Question 2b for linear ression?\r\n",
    "\r\n",
    " \r\n",
    "\r\n",
    "Use the random permutation method to identify the five most important features in the above LASSO model. What are they? Comment on whether the order of their importance is consistent wh your prior.\r\n",
    "\r\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "99a3ec4b-4a4e-4b3d-ab71-a0282f9d831e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for alpha=0.001:\n",
      "Coefficients: [ 8.20646807e+00 -8.56063630e+00 -1.79200698e+00 -5.81679204e-01\n",
      " -1.68631505e+00  3.14439095e+00 -9.86531038e-02  9.60019866e-03\n",
      " -2.90857003e-02  4.35060086e-02  4.41324908e-02 -7.52745426e-03\n",
      " -0.00000000e+00 -0.00000000e+00]\n",
      "Train R-squared: 0.5609841500226345\n",
      "Test R-squared: 0.5579924703758807\n",
      "\n",
      "Results for alpha=0.01:\n",
      "Coefficients: [ 3.68598578 -3.93888988 -0.         -0.46053336 -0.         -0.\n",
      " -0.15572482 -0.         -0.          0.          0.         -0.\n",
      "  0.          0.        ]\n",
      "Train R-squared: 0.38967633167910665\n",
      "Test R-squared: 0.3948203168192118\n",
      "\n",
      "Results for alpha=0.1:\n",
      "Coefficients: [ 0. -0.  0. -0. -0. -0. -0. -0. -0. -0.  0.  0.  0.  0.]\n",
      "Train R-squared: 0.0\n",
      "Test R-squared: -0.0001228637874834959\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#running LASSO\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "alphas = [0.001, 0.01, 0.1]\n",
    "lasso_answers = {}\n",
    "\n",
    "for alpha in alphas:\n",
    "    \n",
    "    lasso_model = Lasso(alpha=alpha, random_state=42)\n",
    "    \n",
    "    \n",
    "    lasso_model.fit(X_train_scaled, Y_train)\n",
    "    \n",
    "    \n",
    "    Y_train_pred = lasso_model.predict(X_train_scaled)\n",
    "    train_r_squared = r2_score(Y_train, Y_train_pred)\n",
    "    \n",
    "    \n",
    "    Y_test_pred = lasso_model.predict(X_test_scaled)\n",
    "    test_r_squared = r2_score(Y_test, Y_test_pred)\n",
    "    \n",
    "    \n",
    "    lasso_answers[alpha] = {\n",
    "        'coefficients': lasso_model.coef_,\n",
    "        'train_r_squared': train_r_squared,\n",
    "        'test_r_squared': test_r_squared\n",
    "    }\n",
    "    print(f\"Results for alpha={alpha}:\")\n",
    "    print(f\"Coefficients: {lasso_model.coef_}\")\n",
    "    print(f\"Train R-squared: {train_r_squared}\")\n",
    "    print(f\"Test R-squared: {test_r_squared}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca58753-685a-43e6-9200-807e47e92fd2",
   "metadata": {},
   "source": [
    "Amongst the penalty parameters i've tried the most appropriate penalty parameter is alpha = 0.001 as maintains complexity of the model and has a R^2 value on the test sample that is close to the OLS regression model in question 2b)\n",
    "\n",
    "Features that have 0 or very close to 0 coefficents are the coefficents that represent my days of the week parameters (coefficents 8 to 14... Day_Monday....Day_Sunday), etc. The first seven coefficents are non zero (SumDailyVisits_Focal, SumDailyVisits_Benchmark, total_population, median_age, housing_units,age_over65,BlueState)This matches with my OLS regression results. The R^2 coefficents are Train R-squared: 0.5609841500226345\r\n",
    "Test R-squared: 0.5579924703758807R-squared in OLS part b): 0.573\n",
    "Its is lower than what we found in Question 2b for linear regression.\n",
    "\n",
    "Use the random permutation method to identify the five most important features in the above LASSO model. What are they? Comment on whether the order of their importance is consistent wh your prior.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c5ae7570-9e75-41a4-b986-30300f1f1c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "\n",
    "best_alpha = 0.001 \n",
    "lasso_best = Lasso(alpha=best_alpha)\n",
    "lasso_best.fit(X_train_scaled, Y_train)\n",
    "\n",
    "\n",
    "perm_importance = permutation_importance(lasso_best, X_test_scaled, Y_test, n_repeats=30, random_state=42)\n",
    "\n",
    "# features sorted by importance\n",
    "sorted_indices = perm_importance.importances_mean.argsort()\n",
    "\n",
    "# names of the top 5 most important features\n",
    "top5_feature_indices = sorted_indices[-5:]\n",
    "top5_feature_names = X.columns[top5_feature_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "cd19a60b-8c1a-4737-bf38-699095f94d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Five most important features according to permutation importance:\n",
      "total_population\n",
      "housing_units\n",
      "age_over65\n",
      "SumDailyVisits_Benchmark\n",
      "SumDailyVisits_Focal\n"
     ]
    }
   ],
   "source": [
    "print(\"Five most important features according to permutation importance:\")\n",
    "for name in top5_feature_names:\n",
    "    print(name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4836bab-775b-4feb-85d0-6f4b67cde31e",
   "metadata": {},
   "source": [
    "Importance is consistent with prior because all these features above have non zero coefficents which show their effect on the Y dependent variable, this effect is consistent with the OLS regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d80d5ff-9c69-4373-ba84-9bf91b95df46",
   "metadata": {},
   "source": [
    "Question 2d (3 points): Using Y and the standardized features in Question 2a, run Ridge in your training sample.\n",
    "\n",
    "Try at least three different values for the penalty parameter. For each value you have tried, report the coefficient estimates and R-squares; also extend the model to predict Y in the test sample, and report the R-squares in the test sample.\n",
    "\n",
    "Among the penalty parameters you have tried, which one is most appropriate? Explain how you arrive at this choice.\n",
    "\n",
    "In the Ridge model with your preferred choice of the penalty parameter, what “features” have zero (or very close to zero) coefficients? What and how many “features” have non-zero coefficients? Comment on whether their signs and magnitude confirm your prior. What is the R-squares when you apply this model to the test sample? Is it higher or lower than what you find in question 3b for linear regression and question 3c for LASSO?\n",
    "\n",
    "Use the random permutation method to identify the five most important features in the above Ridge model. What are they? Comment on whether the order of their importance is consistent with your prior. Does Ridge identify the same five most important features as LASSO?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "3823146f-bb0a-4ec7-9d65-abbbf57a77bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Results for alpha=0.001:\n",
      "Coefficients: [ 9.15989763e+00 -9.02157160e+00 -2.43326040e+00 -8.69227352e-01\n",
      " -4.51635346e+00  6.47631458e+00 -5.61311279e-02  2.41634453e-03\n",
      " -3.57932153e-02  2.85767328e-02  3.22973891e-02 -1.83190969e-02\n",
      " -4.97534836e-03 -4.20280907e-03]\n",
      "Train R-squared: 0.5734989837250057\n",
      "Test R-squared: 0.5704101507296416\n",
      "\n",
      "Ridge Results for alpha=0.01:\n",
      "Coefficients: [ 9.15863669e+00 -9.02084051e+00 -2.43389586e+00 -8.69124337e-01\n",
      " -4.51231781e+00  6.47299539e+00 -5.61566393e-02  2.42060715e-03\n",
      " -3.58028818e-02  2.85894855e-02  3.23091442e-02 -1.83235598e-02\n",
      " -4.98356577e-03 -4.20922971e-03]\n",
      "Train R-squared: 0.5734989654661599\n",
      "Test R-squared: 0.5704119925995126\n",
      "\n",
      "Ridge Results for alpha=0.1:\n",
      "Coefficients: [ 9.14606524e+00 -9.01351451e+00 -2.43995814e+00 -8.68085270e-01\n",
      " -4.47246939e+00  6.44002970e+00 -5.64144243e-02  2.46217952e-03\n",
      " -3.58986125e-02  2.87150915e-02  3.24258762e-02 -1.83674785e-02\n",
      " -5.06457238e-03 -4.27248387e-03]\n",
      "Train R-squared: 0.5734971588020779\n",
      "Test R-squared: 0.5704287566057429\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "alphas = [0.001, 0.01, 0.1]\n",
    "\n",
    "ridge_results = {}\n",
    "\n",
    "for alpha in alphas:\n",
    "    \n",
    "    ridge_model = Ridge(alpha=alpha, random_state=42)\n",
    "    \n",
    "   \n",
    "    ridge_model.fit(X_train_scaled, Y_train)\n",
    "    \n",
    "   \n",
    "    Y_train_pred_ridge = ridge_model.predict(X_train_scaled)\n",
    "    train_r_squared_ridge = r2_score(Y_train, Y_train_pred_ridge)\n",
    "    \n",
    "    \n",
    "    Y_test_pred_ridge = ridge_model.predict(X_test_scaled)\n",
    "    test_r_squared_ridge = r2_score(Y_test, Y_test_pred_ridge)\n",
    "    \n",
    "    \n",
    "    ridge_results[alpha] = {\n",
    "        'coefficients': ridge_model.coef_,\n",
    "        'train_r_squared': train_r_squared_ridge,\n",
    "        'test_r_squared': test_r_squared_ridge\n",
    "    }\n",
    "    \n",
    "    print(f\"Ridge Results for alpha={alpha}:\")\n",
    "    print(f\"Coefficients: {ridge_model.coef_}\")\n",
    "    print(f\"Train R-squared: {train_r_squared_ridge}\")\n",
    "    print(f\"Test R-squared: {test_r_squared_ridge}\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bccc51-a777-479b-b41f-27e721891401",
   "metadata": {},
   "source": [
    "The most appropriate penality parameter is alpha = 0.1 as it shows a very very slight improvement in R^2 value from alpha = 0.001 and alpha = 0.1, showing that it creates the best generalization on unseen test data given patterns learnt from the training data, even though the changes are very small. Also it is closest to OLS regression value found prior .573.\n",
    "\n",
    "Features with alpha = 0.1 that have zero coefficents are consistent with that of the Lasso Model, the days of the week coefficents (coefficents 8 to 14... Day_Monday....Day_Sunday), etc. The first seven coefficents are non zero (SumDailyVisits_Focal, SumDailyVisits_Benchmark, total_population, median_age, housing_units,age_over65,BlueState) which matches with my OLS regression results. The R^2 coefficents are Train R-squared: 0.5734971588020779\n",
    "Test R-squared: 0.5704287566057429\n",
    "R-squared in OLS part b): 0.573\n",
    "R^2 of the test sample in Ridge is higher than the R^2 value found in Lasso but it is still very slightly lower than what we found in Question 2b for linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "186f0b50-c850-4e6d-9897-6ccdf2e1867e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "ridge_best = Ridge(alpha=0.1)\n",
    "ridge_best.fit(X_train_scaled, Y_train)\n",
    "\n",
    "perm_importance_ridge = permutation_importance(ridge_best, X_test_scaled, Y_test, n_repeats=30, random_state=42)\n",
    "\n",
    "\n",
    "important_indices = perm_importance_ridge.importances_mean.argsort()\n",
    "\n",
    "# The five most important features\n",
    "top_5_features = X.columns[important_indices[-5:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "02a5b731-dd39-4a70-81d5-7d21b9172fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Five most important features in the Ridge model are:\n",
      "total_population\n",
      "SumDailyVisits_Benchmark\n",
      "SumDailyVisits_Focal\n",
      "housing_units\n",
      "age_over65\n"
     ]
    }
   ],
   "source": [
    "print(\"Five most important features in the Ridge model are:\")\n",
    "for name in top_5_features:\n",
    "    print(name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396fcd9b-e22d-45f2-84a1-193c7e9c98c8",
   "metadata": {},
   "source": [
    "Although Ridge identifies the same 5 most important features as Lasso, its order of importnace is a little different. They both identify total_population as feature with the most orderof importance (consistent between both models) However, Lasso identifies housing_units and age_over 65 with the second and third most order of importance while Ridge identifies 'SumDailyVisits_Benchmark', 'SumDailyVisits_Focal' as the second and third most order of importance. For Lasso 4th and 5th order of importance is 'SumDailyVisits_Benchmark' and 'SumDailyVisits_Focal' while in the Ridge model the 4th and 5th order of importance is housing_units and age_over65.  Even though both models have the same 5 features as the 5 that have the most order of importnace, the magintude at which each feature affects 5 is different. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
